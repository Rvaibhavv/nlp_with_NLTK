{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rvaib\\.conda\\envs\\toxicsenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import DistilBertTokenizer,BertTokenizer\n",
    "import pandas as pd\n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import tensorflow as tf\n",
    "from transformers import TFDistilBertForSequenceClassification,TFTrainingArguments\n",
    "from transformers import TFTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rvaib\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rvaib\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rvaib\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "# test_x =pd.read_csv('data/test.csv') \n",
    "# test_y =pd.read_csv('data/test_labels.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = pd.merge(test_x,test_y,how='inner',on='id')\n",
    "# test_df =test_df[test_df[\"toxic\"]!=-1].reset_index(drop=True)\n",
    "# test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df =train_df.sample(n=10000,random_state=42)\n",
    "# test_df =test_df.sample(n=2000,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words =set(stopwords.words('english'))\n",
    "lemmatizer =WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens ]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['cleaned_text'] = train_df['comment_text'].apply(preprocess_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119105</th>\n",
       "      <td>7ca72b5b9c688e9e</td>\n",
       "      <td>Geez, are you forgetful!  We've already discus...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>geez , forgetful ! 've already discussed marx ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131631</th>\n",
       "      <td>c03f72fd8f8bf54f</td>\n",
       "      <td>Carioca RFA \\n\\nThanks for your support on my ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>carioca rfa thanks support request adminship ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125326</th>\n",
       "      <td>9e5b8e8fc1ff2e84</td>\n",
       "      <td>\"\\n\\n Birthday \\n\\nNo worries, It's what I do ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>`` birthday worry , 's ; ) enjoy ur day|talk|e ``</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111256</th>\n",
       "      <td>5332799e706665a6</td>\n",
       "      <td>Pseudoscience category? \\n\\nI'm assuming that ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pseudoscience category ? 'm assuming article p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83590</th>\n",
       "      <td>dfa7d8f0b4366680</td>\n",
       "      <td>(and if such phrase exists, it would be provid...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>( phrase exists , would provided search engine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25725</th>\n",
       "      <td>4412b93f983379a4</td>\n",
       "      <td>Sam, get real! No-one's gunner believe all thi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sam , get real ! no-one 's gunner believe stuf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46449</th>\n",
       "      <td>7c1bd75a71704cc2</td>\n",
       "      <td>Get a life \\n\\nwhy dont you go get a life and ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>get life dont go get life stop editing article...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108844</th>\n",
       "      <td>45eed1057ea7a582</td>\n",
       "      <td>Nothing a sock puppet(and trust me, I know wha...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nothing sock puppet ( trust , know mean , sure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142353</th>\n",
       "      <td>f9718cf5cbdd9a25</td>\n",
       "      <td>\"\\nThe lead section is a summary of the main p...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>`` lead section summary main point , mention i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143654</th>\n",
       "      <td>00b548fa0dd148e2</td>\n",
       "      <td>New users \\nDear Mike, \\nit´s not good idea to...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>new user dear mike , it´s good idea block new ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "119105  7ca72b5b9c688e9e  Geez, are you forgetful!  We've already discus...   \n",
       "131631  c03f72fd8f8bf54f  Carioca RFA \\n\\nThanks for your support on my ...   \n",
       "125326  9e5b8e8fc1ff2e84  \"\\n\\n Birthday \\n\\nNo worries, It's what I do ...   \n",
       "111256  5332799e706665a6  Pseudoscience category? \\n\\nI'm assuming that ...   \n",
       "83590   dfa7d8f0b4366680  (and if such phrase exists, it would be provid...   \n",
       "...                  ...                                                ...   \n",
       "25725   4412b93f983379a4  Sam, get real! No-one's gunner believe all thi...   \n",
       "46449   7c1bd75a71704cc2  Get a life \\n\\nwhy dont you go get a life and ...   \n",
       "108844  45eed1057ea7a582  Nothing a sock puppet(and trust me, I know wha...   \n",
       "142353  f9718cf5cbdd9a25  \"\\nThe lead section is a summary of the main p...   \n",
       "143654  00b548fa0dd148e2  New users \\nDear Mike, \\nit´s not good idea to...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "119105      0             0        0       0       0              0   \n",
       "131631      0             0        0       0       0              0   \n",
       "125326      0             0        0       0       0              0   \n",
       "111256      0             0        0       0       0              0   \n",
       "83590       0             0        0       0       0              0   \n",
       "...       ...           ...      ...     ...     ...            ...   \n",
       "25725       0             0        0       0       0              0   \n",
       "46449       1             0        0       0       1              0   \n",
       "108844      0             0        0       0       0              0   \n",
       "142353      0             0        0       0       0              0   \n",
       "143654      0             0        0       0       0              0   \n",
       "\n",
       "                                             cleaned_text  \n",
       "119105  geez , forgetful ! 've already discussed marx ...  \n",
       "131631  carioca rfa thanks support request adminship ....  \n",
       "125326  `` birthday worry , 's ; ) enjoy ur day|talk|e ``  \n",
       "111256  pseudoscience category ? 'm assuming article p...  \n",
       "83590   ( phrase exists , would provided search engine...  \n",
       "...                                                   ...  \n",
       "25725   sam , get real ! no-one 's gunner believe stuf...  \n",
       "46449   get life dont go get life stop editing article...  \n",
       "108844  nothing sock puppet ( trust , know mean , sure...  \n",
       "142353  `` lead section summary main point , mention i...  \n",
       "143654  new user dear mike , it´s good idea block new ...  \n",
       "\n",
       "[10000 rows x 9 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =list(train_df['cleaned_text'])\n",
    "y = train_df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test =train_test_split(X,y, test_size=0.20,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoding =tokenizer(X_train,truncation=True,padding=True)\n",
    "test_encoding =tokenizer(X_test,truncation=True,padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "  dict(train_encoding),y_train  \n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encoding),y_test\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args =TFTrainingArguments(\n",
    "    output_dir='C:/Users/rvaib/OneDrive/Desktop/nlp_with_NLTK/nlp/content/output1',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='/content',\n",
    "    logging_steps=10,\n",
    "    eval_steps=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with training_args.strategy.scope():\n",
    "    model =TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "trainer =TFTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toxicsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
